{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87f7d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwords in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68eb51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리를 수행하는 모듈\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_dict = set(stopwords.words('english'))\n",
    "\n",
    "os.chdir('D:\\study\\Python_JupyterNotebook\\RecommendGame')\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, csv_file_path):\n",
    "        self.df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        self.df['title'] = self.df['title'].fillna('')\n",
    "        columns_to_dropna = ['price', 'req_min', 'req_rec', 'features', 'review']\n",
    "        for column in columns_to_dropna:\n",
    "            self.df.dropna(subset=[column], inplace=True)\n",
    "\n",
    "        self._replace_stopwords()\n",
    "\n",
    "    def _replace_stopwords(self):\n",
    "        def replace_specific_stopwords(text, replacement_dict):\n",
    "            for stopword, replacement in replacement_dict.items():\n",
    "                if stopword in text:\n",
    "                    text = text.replace(stopword, replacement)\n",
    "            return text\n",
    "\n",
    "        self.df['review'] = self.df['review'].apply(\n",
    "            lambda x: replace_specific_stopwords(x, stopwords_dict)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4700a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TextDataVectorizer:\n",
    "    def vectorize_text_data(self, X_train, X_test):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "        X_test_vectorized = vectorizer.transform(X_test)\n",
    "        return X_train_vectorized, X_test_vectorized, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f37deaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_dict = {'씨발': '욕설입니다', '좆': '욕설입니다', '시발': '욕설입니다', '좃': '욕설입니다', '새ㄲㅣ': '욕설입니다',\n",
    "                  '병신': '욕설입니다', '지x': '욕설입니다', '개새끼': '욕설입니다', '좆병신': '욕설입니다','새끼': '욕설입니다',\n",
    "                  '씨2발': '욕설입니다', '족같은': '욕설입니다', '애미뒤진': '욕설입니다', '엄마없음': '욕설입니다','애비': '욕설입니다', 'ㅅ ㅣ ㅂ ㅏ': '욕설입니다', '개좆같고': '욕설입니다', '좆같다': '욕설입니다', 'ㅅㅂ': '욕설입니다',\n",
    "                  'ㅄ': '욕설입니다', 'ㅂㅅ': '욕설입니다', '개 애미': '욕설입니다','개시발': '욕설입니다', '씹새끼들아': '욕설입니다',\n",
    "                  '새끼들': '욕설입니다', '개좆같은게임': '욕설입니다', '씹새끼들아': '욕설입니다','씨팔': '욕설입니다', '개 씨팔': '욕설입니다',\n",
    "                  '씹창': '욕설입니다', '아가리': '욕설입니다','미친년': '욕설입니다', '미친년들아': '욕설입니다',\n",
    "                  '야스': '성행위', '자지': '남성의 생식기','섹스': '성행위', '야스': '성행위', '야스씬': '성행위',\n",
    "                  '보지': '여성의 생식기','자위': '자기위로', '딸딸이': '자기위로', '고추': '남성의 생식기',\n",
    "                  '꼭지': '신체부위','강간': '성범죄', '알몸': '맨몸', '누드': '맨몸', '뷰지': '여성의 생식기',\n",
    "                  '꼭띠': '신체부위', '뷰지': '여성의 생식기',\n",
    "                  '짱개': '중국인유저', '짱깨': '중국인유저', '쪽바리': '일본인유저'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a66b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b975b362",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stopwords_dict' from 'stopwords' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\stopwords\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_vectorizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDataVectorizer\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_preprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPreprocessor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32mD:\\study\\Python_JupyterNotebook\\RecommendGame\\data_preprocessor.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 데이터 전처리를 수행하는 모듈\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstopwords\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataPreprocessor\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file_path):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'stopwords_dict' from 'stopwords' (C:\\Users\\user\\anaconda3\\Lib\\site-packages\\stopwords\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from text_vectorizer import TextDataVectorizer\n",
    "from data_preprocessor import DataPreprocessor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NLTK stopwords 다운로드\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class RecommendationLogic:\n",
    "    \n",
    "    def __init__(self, csv_file_path):\n",
    "        self.preprocessor = DataPreprocessor(csv_file_path)\n",
    "\n",
    "    # LightGBM 모델을 학습\n",
    "    def train_lgbm_model(self, X_train_vectorized, y_train):\n",
    "        lgbm_model = LGBMClassifier()\n",
    "        lgbm_model.fit(X_train_vectorized, y_train)\n",
    "        return lgbm_model\n",
    "    \n",
    "    # 리뷰 데이터 불용어 대체\n",
    "    def _replace_stopwords(self, text, replacement_dict):\n",
    "        for stopword, replacement in replacement_dict.items():\n",
    "            if stopword in text:\n",
    "                text = text.replace(stopword, replacement)\n",
    "        return text\n",
    "\n",
    "    # 게임 추천 해주는 메서드\n",
    "    def recommend_games(self, liked_genres, disliked_genres, model, vectorizer, num_recommendations, max_price, min_review_length, tried_games):\n",
    "        \n",
    "        genre_games = self.preprocessor.df.copy()\n",
    "        \n",
    "        # 좋아하는 장르에 해당하는 게임만 선택\n",
    "        for liked_genre in liked_genres:\n",
    "            genre_games = genre_games[genre_games['tag'].str.contains(liked_genre)]\n",
    "            \n",
    "        # 싫어하는 장르에 게임 제거\n",
    "        if disliked_genres:\n",
    "            for disliked_genre in disliked_genres:\n",
    "                genre_games = genre_games[~genre_games['tag'].str.contains(disliked_genre)]\n",
    "                \n",
    "        # 리뷰 길이가 일정 이상인 게임만 선택\n",
    "        genre_games = genre_games[genre_games['review'].apply(len) >= min_review_length]\n",
    "\n",
    "        # TF-IDF로 텍스트 데이터 벡터화\n",
    "        genre_games_vectorized = vectorizer.transform(genre_games['review'])\n",
    "        \n",
    "        # 각 게임에 대한 예측확률 계산\n",
    "        genre_games['predicted_probability'] = model.predict_proba(genre_games_vectorized)[:, 1]\n",
    "        \n",
    "        # 리뷰에 대한 불용어를 대체하는 작업\n",
    "        stopwords_dict = {word: '' for word in stopwords.words('english')}\n",
    "        genre_games['review'] = genre_games['review'].apply(\n",
    "            lambda x: str(self._replace_stopwords(x, stopwords_dict))\n",
    "        )\n",
    "        \n",
    "        # 예측 확률이 높은 순서로 정렬\n",
    "        recommended_games = genre_games[genre_games['predicted_probability'] > 0.5].sort_values(by='predicted_probability', ascending=False)\n",
    "\n",
    "        # 중복된 게임 제거\n",
    "        recommended_games = recommended_games.drop_duplicates(subset=['title'])\n",
    "        \n",
    "        # 해본 게임 제거        \n",
    "        if tried_games:\n",
    "            recommended_games = recommended_games[~recommended_games['appID'].astype(str).isin(tried_games)]\n",
    "            \n",
    "        # 다양성을 고려하여 상위 게임 선택\n",
    "        selected_games = [] # 최종적으로 선택된 게임 저장\n",
    "        seen_titles = set()\n",
    "        for _, game in recommended_games.iterrows():\n",
    "            title = game['title']\n",
    "            if title not in seen_titles and game['predicted_probability'] > 0.5:\n",
    "                # 가격 정보 추출\n",
    "                prices = game['price']\n",
    "                if not isinstance(prices, str):\n",
    "                    original_price = 'N/A'\n",
    "                else:\n",
    "                    prices = game['price'].split('\\n')\n",
    "                    original_price = prices[-2].strip() if len(prices) > 1 else prices[0].strip()\n",
    "\n",
    "                # 가격이 최대 가격 이하이고 리뷰 길이가 최소 길이 이상인 경우에만 추가\n",
    "                if (original_price == 'Free' or max_price is None or original_price == 'N/A' or float(original_price.replace('₩', '').replace(',', '')) <= max_price) and len(game['review']) >= min_review_length:\n",
    "                    game['price'] = original_price\n",
    "                    selected_games.append(game)\n",
    "                    seen_titles.add(title)\n",
    "\n",
    "                # 원하는 추천 수에 도달하면 종료\n",
    "                if len(selected_games) == num_recommendations:\n",
    "                    break\n",
    "\n",
    "        # 추천된 게임 목록에서 appID 반환\n",
    "        recommendations = pd.DataFrame(selected_games)[['appID']]\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c32eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'recommendation_logic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrecommendation_logic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecommendationLogic\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_vectorizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextDataVectorizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'recommendation_logic'"
     ]
    }
   ],
   "source": [
    "#%% main\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.model_selection import train_test_split\n",
    "from recommendation_logic import RecommendationLogic\n",
    "from text_vectorizer import TextDataVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir('D:\\study\\Python_JupyterNotebook\\RecommendGame') # 크롤링 데이터 파일 있는 곳으로 변경\n",
    "\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 전역으로 모델과 벡터라이저 초기화\n",
    "csv_file_path = '스팀 크롤링 데이터(2024-01-13)전체.csv'\n",
    "recommendation_logic = RecommendationLogic(csv_file_path)\n",
    "text_vectorizer = TextDataVectorizer()\n",
    "\n",
    "# 데이터 전처리\n",
    "# price, req_min, req_rec, features, review 가 없으면 삭제\n",
    "# 리뷰데이터에  불용어를 다른 단어로 대체 예시(시발 = 욕설입니다)\n",
    "recommendation_logic.preprocessor.preprocess_data()\n",
    "\n",
    "# 데이터를 훈련 세트와 테스트 세트로 나누는 작업\n",
    "# 기계 학습 모델을 훈련시키고 성능을 평가하기 위해 나눔\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    recommendation_logic.preprocessor.df[['title', 'appID', 'price', 'tag', 'review']].astype(str),\n",
    "    recommendation_logic.preprocessor.df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터에 대해 텍스트 데이터를 벡터화하는 과정\n",
    "# 기계 학습에 사용할 수 있게 텍스트를 수치로 변환\n",
    "X_train_vectorized, X_test_vectorized, vectorizer = text_vectorizer.vectorize_text_data(X_train['review'], X_test['review'])\n",
    "\n",
    "# lgbm 모델 학습\n",
    "lgbm_model = recommendation_logic.train_lgbm_model(X_train_vectorized, y_train)\n",
    "\n",
    "# 스프링부트 에서 json데이터 받아와서 처리하는 부분\n",
    "@app.route('/', methods=['POST'])\n",
    "def get_recommendations():\n",
    "    try:\n",
    "        # json 데이터 받아오는 부분\n",
    "        # force=True force=True를 사용하여 Content-Type을 강제로 JSON으로 처리\n",
    "        request_data = request.get_json() \n",
    "        \n",
    "        \n",
    "        liked_genres = request_data.get('Want Category', [])    # 좋아하는 게임 장르\n",
    "        disliked_genres = request_data.get('Hate Category', []) # 싫어하는 게임 장르\n",
    "        tried_games = request_data.get('Try Game', [])          # 해본 게임\n",
    "\n",
    "        \n",
    "        # 게임 추천 기능 실행\n",
    "        recommendations = recommendation_logic.recommend_games(\n",
    "            liked_genres,     # 좋아하는 게임 장르\n",
    "            disliked_genres,  # 싫어하는 게임 장르\n",
    "            lgbm_model,       # lgbm모델 적용\n",
    "            vectorizer,       # 벡터라이즈 적용\n",
    "            10, 100000, 1000, # 게임추천수 10개, 최대가격 100000원, 리뷰길이 1000 이상\n",
    "            tried_games       # 해본 게임\n",
    "        )\n",
    "\n",
    "        # 결과를 문자열 형식으로 응답\n",
    "        result_text = ','.join(recommendations['appID'].astype(str))\n",
    "        print(\"Recommendations:\", result_text)\n",
    "        return result_text\n",
    "    \n",
    "    # 오류 메시지 출력\n",
    "    except Exception as e:\n",
    "        error_message = {'error': str(e)}\n",
    "        print(\"Error:\", error_message)\n",
    "        return jsonify(error_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13ff913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://211.208.84.145:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [11/Mar/2024 18:45:54] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations: 1372810,518030,1551360,564230,518790,678950,495570,359550,2373390,1222680\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
